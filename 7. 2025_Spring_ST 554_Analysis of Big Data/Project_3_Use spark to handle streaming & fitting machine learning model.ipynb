{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8292ec54-f032-431d-bdf9-f5ff2118fcf1",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe11e3-defe-4b3e-a24c-2bd135111d31",
   "metadata": {},
   "source": [
    "## 1. Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6104499-4d94-4c2c-ae8d-48736fabab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccd27ac2-120a-4e70-a124-4c18ee1f506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Binarizer, StringIndexer, OneHotEncoder, VectorAssembler, PCA\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3536dc02-c051-429c-a58a-bdd2fc2fdbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/28 14:01:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/28 14:01:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Load the data file\n",
    "df_pd = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/power_ml_data.csv\")\n",
    "\n",
    "# Convert the pandas DataFrame to a pandas-on-Spark DataFrame\n",
    "df_sp = ps.from_pandas(df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef083df1-a92d-4ede-b0b0-5295e02f81f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-1. Summarize the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee05886-bf67-4931-a8c4-8ce9bd192ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 47174 entries, 0 to 47173\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Temperature            47174 non-null  float64\n",
      " 1   Humidity               47174 non-null  float64\n",
      " 2   Wind_Speed             47174 non-null  float64\n",
      " 3   General_Diffuse_Flows  47174 non-null  float64\n",
      " 4   Diffuse_Flows          47174 non-null  float64\n",
      " 5   Power_Zone_1           47174 non-null  float64\n",
      " 6   Power_Zone_2           47174 non-null  float64\n",
      " 7   Power_Zone_3           47174 non-null  float64\n",
      " 8   Month                  47174 non-null  int64  \n",
      " 9   Hour                   47174 non-null  int64  \n",
      "dtypes: float64(8), int64(2)"
     ]
    }
   ],
   "source": [
    "# Display basic information\n",
    "df_sp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69aa5a3-7aad-4396-a46a-b0f145de04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 14:02:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "      <th>Month</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "      <td>47174.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.813220</td>\n",
       "      <td>68.288398</td>\n",
       "      <td>1.961621</td>\n",
       "      <td>182.531180</td>\n",
       "      <td>74.987211</td>\n",
       "      <td>32335.168690</td>\n",
       "      <td>21027.204976</td>\n",
       "      <td>17831.197608</td>\n",
       "      <td>6.510599</td>\n",
       "      <td>11.488383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.813341</td>\n",
       "      <td>15.560330</td>\n",
       "      <td>2.349351</td>\n",
       "      <td>264.431856</td>\n",
       "      <td>124.256146</td>\n",
       "      <td>7130.013305</td>\n",
       "      <td>5199.787153</td>\n",
       "      <td>6622.590470</td>\n",
       "      <td>3.437367</td>\n",
       "      <td>6.921920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.247000</td>\n",
       "      <td>11.340000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>13895.696200</td>\n",
       "      <td>8560.081466</td>\n",
       "      <td>5935.174070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.420000</td>\n",
       "      <td>58.320000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>26290.632910</td>\n",
       "      <td>16957.317070</td>\n",
       "      <td>13121.927710</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.780000</td>\n",
       "      <td>69.890000</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>4.284000</td>\n",
       "      <td>32261.596960</td>\n",
       "      <td>20804.863220</td>\n",
       "      <td>16405.282110</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22.910000</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>4.915000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>37317.446810</td>\n",
       "      <td>24698.734180</td>\n",
       "      <td>21628.915660</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.010000</td>\n",
       "      <td>94.800000</td>\n",
       "      <td>6.483000</td>\n",
       "      <td>1163.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>52146.859050</td>\n",
       "      <td>37408.860760</td>\n",
       "      <td>47598.326360</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Temperature      Humidity    Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  Power_Zone_1  Power_Zone_2  Power_Zone_3         Month          Hour\n",
       "count  47174.000000  47174.000000  47174.000000           47174.000000   47174.000000  47174.000000  47174.000000  47174.000000  47174.000000  47174.000000\n",
       "mean      18.813220     68.288398      1.961621             182.531180      74.987211  32335.168690  21027.204976  17831.197608      6.510599     11.488383\n",
       "std        5.813341     15.560330      2.349351             264.431856     124.256146   7130.013305   5199.787153   6622.590470      3.437367      6.921920\n",
       "min        3.247000     11.340000      0.050000               0.004000       0.011000  13895.696200   8560.081466   5935.174070      1.000000      0.000000\n",
       "25%       14.420000     58.320000      0.078000               0.062000       0.122000  26290.632910  16957.317070  13121.927710      4.000000      5.000000\n",
       "50%       18.780000     69.890000      0.086000               4.780000       4.284000  32261.596960  20804.863220  16405.282110      7.000000     11.000000\n",
       "75%       22.910000     81.500000      4.915000             319.000000     101.000000  37317.446810  24698.734180  21628.915660      9.000000     17.000000\n",
       "max       40.010000     94.800000      6.483000            1163.000000     936.000000  52146.859050  37408.860760  47598.326360     12.000000     23.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the means, standard deviations, min, max\n",
    "# Use pandas-on-Spark `describe()` to quickly generate summary statistics for all variables\n",
    "df_sp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57963ed5-e613-469d-a946-db99a2cf4eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Temperature                 18.78000\n",
       "Humidity                    69.89000\n",
       "Wind_Speed                   0.08600\n",
       "General_Diffuse_Flows        4.78000\n",
       "Diffuse_Flows                4.28400\n",
       "Power_Zone_1             32261.59696\n",
       "Power_Zone_2             20804.86322\n",
       "Power_Zone_3             16405.28211\n",
       "Month                        7.00000\n",
       "Hour                        11.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the median\n",
    "# We can use `median()` directly as in pandas to compute the median for each column\n",
    "df_sp.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ca914-b38e-45f2-aa72-e7b0dc0ca83a",
   "metadata": {},
   "source": [
    "* The mean values of General_Diffuse_Flows and Diffuse_Flows are significantly higher than their median values. \n",
    "* This indicates that the majority of the data points for these two variables are concentrated at lower values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4df8fdb-ecf4-47a2-8b62-549bcff4c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===============>                                      (36 + 92) / 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Temperature  Humidity  Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  Power_Zone_1  Power_Zone_2  Power_Zone_3     Month      Hour\n",
      "Temperature               1.000000 -0.460143    0.476421               0.459602       0.195625      0.441446      0.384301      0.490752  0.284814  0.199134\n",
      "Humidity                 -0.460143  1.000000   -0.136121              -0.467282      -0.258042     -0.289090     -0.297019     -0.234228 -0.016762 -0.244204\n",
      "Wind_Speed                0.476421 -0.136121    1.000000               0.132304      -0.000727      0.166322      0.146338      0.279112  0.168491  0.004289\n",
      "General_Diffuse_Flows     0.459602 -0.467282    0.132304               1.000000       0.564530      0.189994      0.158798      0.064942 -0.020793  0.131171\n",
      "Diffuse_Flows             0.195625 -0.258042   -0.000727               0.564530       1.000000      0.082885      0.047379     -0.036761 -0.130249  0.132257\n",
      "Power_Zone_1              0.441446 -0.289090    0.166322               0.189994       0.082885      1.000000      0.834694      0.750656 -0.006429  0.728118\n",
      "Power_Zone_2              0.384301 -0.297019    0.146338               0.158798       0.047379      0.834694      1.000000      0.572344  0.318368  0.663755\n",
      "Power_Zone_3              0.490752 -0.234228    0.279112               0.064942      -0.036761      0.750656      0.572344      1.000000 -0.232978  0.454806\n",
      "Month                     0.284814 -0.016762    0.168491              -0.020793      -0.130249     -0.006429      0.318368     -0.232978  1.000000 -0.000419\n",
      "Hour                      0.199134 -0.244204    0.004289               0.131171       0.132257      0.728118      0.663755      0.454806 -0.000419  1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the correlations between all variables\n",
    "# We can use `.corr()` to analyze correlations between multiple variables\n",
    "corr_matrix = df_sp.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb879164-b8b1-4f6a-8a8a-a1c721eeb604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     4014\n",
       "2     3588\n",
       "3     4057\n",
       "4     3893\n",
       "5     3997\n",
       "6     3913\n",
       "7     4029\n",
       "8     3999\n",
       "9     3913\n",
       "10    4026\n",
       "11    3877\n",
       "12    3868\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a one-way contingency table of the Month variable\n",
    "# Use `.value_counts()` to calculate the frequency of each value,\n",
    "# and `.sort_index()` to sort the result by increasing the index values\n",
    "df_sp[\"Month\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa18276-3154-49e2-9065-0c7f231b95b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     1950\n",
       "1     1973\n",
       "2     1973\n",
       "3     1966\n",
       "4     1986\n",
       "5     1968\n",
       "6     1992\n",
       "7     1964\n",
       "8     1957\n",
       "9     1976\n",
       "10    1955\n",
       "11    1972\n",
       "12    1979\n",
       "13    1956\n",
       "14    1971\n",
       "15    1947\n",
       "16    1950\n",
       "17    1979\n",
       "18    1955\n",
       "19    1950\n",
       "20    1945\n",
       "21    1976\n",
       "22    1966\n",
       "23    1968\n",
       "Name: Hour, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Create a one-way contingency table of the Hour variable\n",
    "# Use `.value_counts()` to calculate the frequency of each value,\n",
    "# and `.sort_index()` to sort the result by increasing the index values\n",
    "df_sp[\"Hour\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2052036a-ff1d-450e-b976-457d4c787dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month  Hour\n",
       "1      0       161\n",
       "       1       165\n",
       "       2       168\n",
       "       3       162\n",
       "       4       167\n",
       "       5       168\n",
       "       6       168\n",
       "       7       168\n",
       "       8       172\n",
       "       9       174\n",
       "       10      164\n",
       "       11      174\n",
       "       12      170\n",
       "       13      165\n",
       "       14      167\n",
       "       15      163\n",
       "       16      169\n",
       "       17      170\n",
       "       18      168\n",
       "       19      162\n",
       "       20      172\n",
       "       21      166\n",
       "       22      166\n",
       "       23      165\n",
       "2      0       142\n",
       "       1       151\n",
       "       2       147\n",
       "       3       148\n",
       "       4       156\n",
       "       5       147\n",
       "       6       157\n",
       "       7       153\n",
       "       8       147\n",
       "       9       153\n",
       "       10      145\n",
       "       11      141\n",
       "       12      155\n",
       "       13      154\n",
       "       14      153\n",
       "       15      146\n",
       "       16      153\n",
       "       17      152\n",
       "       18      144\n",
       "       19      148\n",
       "       20      149\n",
       "       21      151\n",
       "       22      151\n",
       "       23      145\n",
       "3      0       174\n",
       "       1       171\n",
       "       2       167\n",
       "       3       168\n",
       "       4       166\n",
       "       5       170\n",
       "       6       173\n",
       "       7       170\n",
       "       8       166\n",
       "       9       170\n",
       "       10      175\n",
       "       11      168\n",
       "       12      166\n",
       "       13      173\n",
       "       14      165\n",
       "       15      171\n",
       "       16      166\n",
       "       17      163\n",
       "       18      165\n",
       "       19      170\n",
       "       20      168\n",
       "       21      175\n",
       "       22      165\n",
       "       23      172\n",
       "4      0       161\n",
       "       1       162\n",
       "       2       157\n",
       "       3       160\n",
       "       4       163\n",
       "       5       169\n",
       "       6       167\n",
       "       7       162\n",
       "       8       165\n",
       "       9       166\n",
       "       10      164\n",
       "       11      163\n",
       "       12      165\n",
       "       13      157\n",
       "       14      165\n",
       "       15      156\n",
       "       16      162\n",
       "       17      161\n",
       "       18      155\n",
       "       19      163\n",
       "       20      161\n",
       "       21      165\n",
       "       22      160\n",
       "       23      164\n",
       "5      0       164\n",
       "       1       166\n",
       "       2       163\n",
       "       3       171\n",
       "       4       174\n",
       "       5       164\n",
       "       6       172\n",
       "       7       163\n",
       "       8       167\n",
       "       9       168\n",
       "       10      172\n",
       "       11      159\n",
       "       12      175\n",
       "       13      166\n",
       "       14      159\n",
       "       15      172\n",
       "       16      167\n",
       "       17      171\n",
       "       18      162\n",
       "       19      160\n",
       "       20      165\n",
       "       21      174\n",
       "       22      161\n",
       "       23      162\n",
       "6      0       160\n",
       "       1       164\n",
       "       2       169\n",
       "       3       165\n",
       "       4       167\n",
       "       5       167\n",
       "       6       163\n",
       "       7       167\n",
       "       8       160\n",
       "       9       164\n",
       "       10      159\n",
       "       11      169\n",
       "       12      155\n",
       "       13      167\n",
       "       14      162\n",
       "       15      158\n",
       "       16      161\n",
       "       17      161\n",
       "       18      165\n",
       "       19      162\n",
       "       20      159\n",
       "       21      156\n",
       "       22      167\n",
       "       23      166\n",
       "7      0       172\n",
       "       1       169\n",
       "       2       161\n",
       "       3       170\n",
       "       4       167\n",
       "       5       174\n",
       "       6       168\n",
       "       7       164\n",
       "       8       165\n",
       "       9       168\n",
       "       10      167\n",
       "       11      168\n",
       "       12      166\n",
       "       13      173\n",
       "       14      169\n",
       "       15      162\n",
       "       16      164\n",
       "       17      172\n",
       "       18      176\n",
       "       19      172\n",
       "       20      159\n",
       "       21      163\n",
       "       22      173\n",
       "       23      167\n",
       "8      0       168\n",
       "       1       169\n",
       "       2       171\n",
       "       3       165\n",
       "       4       168\n",
       "       5       171\n",
       "       6       165\n",
       "       7       162\n",
       "       8       165\n",
       "       9       157\n",
       "       10      162\n",
       "       11      171\n",
       "       12      167\n",
       "       13      167\n",
       "       14      164\n",
       "       15      166\n",
       "       16      168\n",
       "       17      165\n",
       "       18      169\n",
       "       19      168\n",
       "       20      165\n",
       "       21      169\n",
       "       22      164\n",
       "       23      173\n",
       "9      0       164\n",
       "       1       161\n",
       "       2       160\n",
       "       3       164\n",
       "       4       167\n",
       "       5       158\n",
       "       6       162\n",
       "       7       159\n",
       "       8       162\n",
       "       9       162\n",
       "       10      159\n",
       "       11      161\n",
       "       12      174\n",
       "       13      159\n",
       "       14      162\n",
       "       15      168\n",
       "       16      166\n",
       "       17      169\n",
       "       18      162\n",
       "       19      163\n",
       "       20      158\n",
       "       21      163\n",
       "       22      163\n",
       "       23      167\n",
       "10     0       171\n",
       "       1       168\n",
       "       2       175\n",
       "       3       172\n",
       "       4       162\n",
       "       5       163\n",
       "       6       170\n",
       "       7       164\n",
       "       8       168\n",
       "       9       171\n",
       "       10      163\n",
       "       11      171\n",
       "       12      166\n",
       "       13      161\n",
       "       14      175\n",
       "       15      165\n",
       "       16      159\n",
       "       17      176\n",
       "       18      165\n",
       "       19      162\n",
       "       20      170\n",
       "       21      173\n",
       "       22      169\n",
       "       23      167\n",
       "11     0       159\n",
       "       1       168\n",
       "       2       168\n",
       "       3       154\n",
       "       4       165\n",
       "       5       153\n",
       "       6       162\n",
       "       7       167\n",
       "       8       164\n",
       "       9       164\n",
       "       10      160\n",
       "       11      160\n",
       "       12      156\n",
       "       13      158\n",
       "       14      162\n",
       "       15      163\n",
       "       16      157\n",
       "       17      166\n",
       "       18      165\n",
       "       19      160\n",
       "       20      161\n",
       "       21      162\n",
       "       22      159\n",
       "       23      164\n",
       "12     0       154\n",
       "       1       159\n",
       "       2       167\n",
       "       3       167\n",
       "       4       164\n",
       "       5       164\n",
       "       6       165\n",
       "       7       165\n",
       "       8       156\n",
       "       9       159\n",
       "       10      165\n",
       "       11      167\n",
       "       12      164\n",
       "       13      156\n",
       "       14      168\n",
       "       15      157\n",
       "       16      158\n",
       "       17      153\n",
       "       18      159\n",
       "       19      160\n",
       "       20      158\n",
       "       21      159\n",
       "       22      168\n",
       "       23      156\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a two-way contingency table for the Month and Hour variables\n",
    "# Group the data by the combination of \"Month\" and \"Hour\"\n",
    "# Use '.size()' to count the number of rows in each group\n",
    "# Use '.sort_index()' to sort the groups by index order\n",
    "df_sp.groupby([\"Month\", \"Hour\"]).size().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bffa0ac1-3782-4038-a64b-c33a19b74c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages/pyspark/pandas/groupby.py:649: FutureWarning: Default value of `numeric_only` will be changed to `False` instead of `True` in 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.734699</td>\n",
       "      <td>68.258548</td>\n",
       "      <td>0.702223</td>\n",
       "      <td>103.959658</td>\n",
       "      <td>69.798826</td>\n",
       "      <td>31052.984428</td>\n",
       "      <td>19407.916366</td>\n",
       "      <td>17736.351685</td>\n",
       "      <td>11.512706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.656535</td>\n",
       "      <td>66.490925</td>\n",
       "      <td>1.113977</td>\n",
       "      <td>125.471135</td>\n",
       "      <td>92.330615</td>\n",
       "      <td>30973.863160</td>\n",
       "      <td>18774.586006</td>\n",
       "      <td>17309.707870</td>\n",
       "      <td>11.497492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.584055</td>\n",
       "      <td>71.115884</td>\n",
       "      <td>1.006017</td>\n",
       "      <td>181.401719</td>\n",
       "      <td>93.155905</td>\n",
       "      <td>31162.869031</td>\n",
       "      <td>18459.612113</td>\n",
       "      <td>16945.462800</td>\n",
       "      <td>11.479172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.414755</td>\n",
       "      <td>75.408176</td>\n",
       "      <td>0.222990</td>\n",
       "      <td>157.722243</td>\n",
       "      <td>83.494537</td>\n",
       "      <td>31143.206766</td>\n",
       "      <td>17600.306571</td>\n",
       "      <td>18574.918338</td>\n",
       "      <td>11.482147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.301401</td>\n",
       "      <td>68.609322</td>\n",
       "      <td>2.307473</td>\n",
       "      <td>274.500026</td>\n",
       "      <td>122.765576</td>\n",
       "      <td>32379.460464</td>\n",
       "      <td>19973.085387</td>\n",
       "      <td>17604.282564</td>\n",
       "      <td>11.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22.132706</td>\n",
       "      <td>68.761260</td>\n",
       "      <td>1.561346</td>\n",
       "      <td>277.434533</td>\n",
       "      <td>103.227789</td>\n",
       "      <td>34573.227026</td>\n",
       "      <td>20649.034590</td>\n",
       "      <td>20416.130091</td>\n",
       "      <td>11.456683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27.200593</td>\n",
       "      <td>57.599484</td>\n",
       "      <td>4.641782</td>\n",
       "      <td>294.112037</td>\n",
       "      <td>75.410538</td>\n",
       "      <td>35805.530436</td>\n",
       "      <td>24130.028182</td>\n",
       "      <td>28175.034099</td>\n",
       "      <td>11.495656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.740415</td>\n",
       "      <td>66.022621</td>\n",
       "      <td>4.533251</td>\n",
       "      <td>227.178635</td>\n",
       "      <td>67.105847</td>\n",
       "      <td>36436.261651</td>\n",
       "      <td>24657.024552</td>\n",
       "      <td>24684.368961</td>\n",
       "      <td>11.506627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.640565</td>\n",
       "      <td>66.868306</td>\n",
       "      <td>2.947096</td>\n",
       "      <td>202.201634</td>\n",
       "      <td>49.070622</td>\n",
       "      <td>33415.103456</td>\n",
       "      <td>20189.459837</td>\n",
       "      <td>14928.415530</td>\n",
       "      <td>11.532328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.476249</td>\n",
       "      <td>71.524016</td>\n",
       "      <td>2.784221</td>\n",
       "      <td>115.814556</td>\n",
       "      <td>46.628719</td>\n",
       "      <td>32806.992796</td>\n",
       "      <td>21457.890001</td>\n",
       "      <td>13266.437337</td>\n",
       "      <td>11.486587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.819408</td>\n",
       "      <td>69.638342</td>\n",
       "      <td>1.258902</td>\n",
       "      <td>121.915119</td>\n",
       "      <td>62.876133</td>\n",
       "      <td>28993.342196</td>\n",
       "      <td>23229.117470</td>\n",
       "      <td>12867.073100</td>\n",
       "      <td>11.492133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.283378</td>\n",
       "      <td>69.237386</td>\n",
       "      <td>0.255324</td>\n",
       "      <td>100.023251</td>\n",
       "      <td>34.305382</td>\n",
       "      <td>28959.102866</td>\n",
       "      <td>23618.564516</td>\n",
       "      <td>11017.120312</td>\n",
       "      <td>11.452430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temperature   Humidity  Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  Power_Zone_1  Power_Zone_2  Power_Zone_3       Hour\n",
       "Month                                                                                                                               \n",
       "1        12.734699  68.258548    0.702223             103.959658      69.798826  31052.984428  19407.916366  17736.351685  11.512706\n",
       "2        12.656535  66.490925    1.113977             125.471135      92.330615  30973.863160  18774.586006  17309.707870  11.497492\n",
       "3        14.584055  71.115884    1.006017             181.401719      93.155905  31162.869031  18459.612113  16945.462800  11.479172\n",
       "4        16.414755  75.408176    0.222990             157.722243      83.494537  31143.206766  17600.306571  18574.918338  11.482147\n",
       "5        20.301401  68.609322    2.307473             274.500026     122.765576  32379.460464  19973.085387  17604.282564  11.466600\n",
       "6        22.132706  68.761260    1.561346             277.434533     103.227789  34573.227026  20649.034590  20416.130091  11.456683\n",
       "7        27.200593  57.599484    4.641782             294.112037      75.410538  35805.530436  24130.028182  28175.034099  11.495656\n",
       "8        25.740415  66.022621    4.533251             227.178635      67.105847  36436.261651  24657.024552  24684.368961  11.506627\n",
       "9        22.640565  66.868306    2.947096             202.201634      49.070622  33415.103456  20189.459837  14928.415530  11.532328\n",
       "10       20.476249  71.524016    2.784221             115.814556      46.628719  32806.992796  21457.890001  13266.437337  11.486587\n",
       "11       16.819408  69.638342    1.258902             121.915119      62.876133  28993.342196  23229.117470  12867.073100  11.492133\n",
       "12       13.283378  69.237386    0.255324             100.023251      34.305382  28959.102866  23618.564516  11017.120312  11.452430"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by Month and find the means of the numeric variables\n",
    "# Group the data by \"Month\" column\n",
    "# Use '.mean()' to calculate the mean for each group\n",
    "df_sp.groupby(\"Month\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f41da55c-28b9-4664-99c2-60301bab03fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.240635</td>\n",
       "      <td>12.156170</td>\n",
       "      <td>1.611795</td>\n",
       "      <td>166.164710</td>\n",
       "      <td>131.459172</td>\n",
       "      <td>7402.323411</td>\n",
       "      <td>4515.295696</td>\n",
       "      <td>4436.997405</td>\n",
       "      <td>6.893386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.619715</td>\n",
       "      <td>12.411942</td>\n",
       "      <td>1.981157</td>\n",
       "      <td>206.730180</td>\n",
       "      <td>169.155517</td>\n",
       "      <td>6874.584791</td>\n",
       "      <td>4390.391101</td>\n",
       "      <td>4353.975946</td>\n",
       "      <td>6.900281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.758852</td>\n",
       "      <td>13.918146</td>\n",
       "      <td>1.900982</td>\n",
       "      <td>260.148889</td>\n",
       "      <td>151.167923</td>\n",
       "      <td>6782.136539</td>\n",
       "      <td>4185.117595</td>\n",
       "      <td>4256.765546</td>\n",
       "      <td>6.934686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.806221</td>\n",
       "      <td>14.312655</td>\n",
       "      <td>0.820343</td>\n",
       "      <td>246.173501</td>\n",
       "      <td>123.912352</td>\n",
       "      <td>6496.700167</td>\n",
       "      <td>3835.629384</td>\n",
       "      <td>4556.263192</td>\n",
       "      <td>6.913948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.299952</td>\n",
       "      <td>16.436023</td>\n",
       "      <td>2.408328</td>\n",
       "      <td>331.998897</td>\n",
       "      <td>171.585943</td>\n",
       "      <td>6809.332811</td>\n",
       "      <td>4182.543672</td>\n",
       "      <td>4353.394234</td>\n",
       "      <td>6.904736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.689724</td>\n",
       "      <td>14.972905</td>\n",
       "      <td>2.235411</td>\n",
       "      <td>328.277210</td>\n",
       "      <td>143.497896</td>\n",
       "      <td>7317.808097</td>\n",
       "      <td>4465.664309</td>\n",
       "      <td>5596.702926</td>\n",
       "      <td>6.933494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.856707</td>\n",
       "      <td>18.849986</td>\n",
       "      <td>1.110539</td>\n",
       "      <td>331.733608</td>\n",
       "      <td>95.044517</td>\n",
       "      <td>6966.074191</td>\n",
       "      <td>4968.511101</td>\n",
       "      <td>6913.958361</td>\n",
       "      <td>6.927293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.949887</td>\n",
       "      <td>18.482551</td>\n",
       "      <td>1.300900</td>\n",
       "      <td>289.906192</td>\n",
       "      <td>90.662540</td>\n",
       "      <td>7054.722275</td>\n",
       "      <td>5163.442765</td>\n",
       "      <td>6520.955955</td>\n",
       "      <td>6.950723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.877585</td>\n",
       "      <td>15.992826</td>\n",
       "      <td>2.293412</td>\n",
       "      <td>270.172564</td>\n",
       "      <td>67.523539</td>\n",
       "      <td>6471.366900</td>\n",
       "      <td>4205.504219</td>\n",
       "      <td>3425.856320</td>\n",
       "      <td>6.922190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.987293</td>\n",
       "      <td>13.980791</td>\n",
       "      <td>2.398703</td>\n",
       "      <td>185.043298</td>\n",
       "      <td>69.421308</td>\n",
       "      <td>6479.133482</td>\n",
       "      <td>4612.671175</td>\n",
       "      <td>3087.805818</td>\n",
       "      <td>6.944604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.683225</td>\n",
       "      <td>12.832011</td>\n",
       "      <td>2.075377</td>\n",
       "      <td>184.692025</td>\n",
       "      <td>123.141505</td>\n",
       "      <td>5918.582563</td>\n",
       "      <td>5452.892158</td>\n",
       "      <td>3509.671954</td>\n",
       "      <td>6.933768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.348619</td>\n",
       "      <td>13.739463</td>\n",
       "      <td>0.893755</td>\n",
       "      <td>161.702441</td>\n",
       "      <td>56.393569</td>\n",
       "      <td>6176.465110</td>\n",
       "      <td>5713.467069</td>\n",
       "      <td>2840.343209</td>\n",
       "      <td>6.910306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temperature   Humidity  Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  Power_Zone_1  Power_Zone_2  Power_Zone_3      Hour\n",
       "Month                                                                                                                              \n",
       "1         3.240635  12.156170    1.611795             166.164710     131.459172   7402.323411   4515.295696   4436.997405  6.893386\n",
       "2         2.619715  12.411942    1.981157             206.730180     169.155517   6874.584791   4390.391101   4353.975946  6.900281\n",
       "3         3.758852  13.918146    1.900982             260.148889     151.167923   6782.136539   4185.117595   4256.765546  6.934686\n",
       "4         2.806221  14.312655    0.820343             246.173501     123.912352   6496.700167   3835.629384   4556.263192  6.913948\n",
       "5         3.299952  16.436023    2.408328             331.998897     171.585943   6809.332811   4182.543672   4353.394234  6.904736\n",
       "6         2.689724  14.972905    2.235411             328.277210     143.497896   7317.808097   4465.664309   5596.702926  6.933494\n",
       "7         3.856707  18.849986    1.110539             331.733608      95.044517   6966.074191   4968.511101   6913.958361  6.927293\n",
       "8         2.949887  18.482551    1.300900             289.906192      90.662540   7054.722275   5163.442765   6520.955955  6.950723\n",
       "9         2.877585  15.992826    2.293412             270.172564      67.523539   6471.366900   4205.504219   3425.856320  6.922190\n",
       "10        2.987293  13.980791    2.398703             185.043298      69.421308   6479.133482   4612.671175   3087.805818  6.944604\n",
       "11        3.683225  12.832011    2.075377             184.692025     123.141505   5918.582563   5452.892158   3509.671954  6.933768\n",
       "12        3.348619  13.739463    0.893755             161.702441      56.393569   6176.465110   5713.467069   2840.343209  6.910306"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by Month and find the standard deviations of the numeric variables\n",
    "# Group the data by \"Month\" column\n",
    "# Use '.std()' to calculate the standard deviation for each group\n",
    "df_sp.groupby(\"Month\").std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6dc2f6-01db-4a9d-8473-c85b9a721902",
   "metadata": {},
   "source": [
    "### 1-2. Switch to Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee3ad9e-8ce1-44eb-a715-3fac23dc4deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark2/lib/python3.9/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame from pandas-on-Spark to a Spark SQL DataFrame\n",
    "df_sql = df_sp.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be77094-8f39-4b61-aad8-a88b5e454ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temperature: double (nullable = false)\n",
      " |-- Humidity: double (nullable = false)\n",
      " |-- Wind_Speed: double (nullable = false)\n",
      " |-- General_Diffuse_Flows: double (nullable = false)\n",
      " |-- Diffuse_Flows: double (nullable = false)\n",
      " |-- Power_Zone_1: double (nullable = false)\n",
      " |-- Power_Zone_2: double (nullable = false)\n",
      " |-- Power_Zone_3: double (nullable = false)\n",
      " |-- Month: long (nullable = false)\n",
      " |-- Hour: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the DataFrame\n",
    "df_sql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5d5b35-3eae-490b-bc6b-7c93c9de1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use '.withColumn()' to overwrite existing columns with new types\n",
    "# Use '.cast()' to convert the data type of a column; here, 'long' is cast to 'double'\n",
    "df_sql = df_sql.withColumn(\"Month\", col(\"Month\").cast(\"double\")) \\\n",
    "               .withColumn(\"Hour\", col(\"Hour\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3900cfbc-2fef-4d85-82ce-93afa2f20228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temperature: double (nullable = false)\n",
      " |-- Humidity: double (nullable = false)\n",
      " |-- Wind_Speed: double (nullable = false)\n",
      " |-- General_Diffuse_Flows: double (nullable = false)\n",
      " |-- Diffuse_Flows: double (nullable = false)\n",
      " |-- Power_Zone_1: double (nullable = false)\n",
      " |-- Power_Zone_2: double (nullable = false)\n",
      " |-- Power_Zone_3: double (nullable = false)\n",
      " |-- Month: double (nullable = false)\n",
      " |-- Hour: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recheck the schema to confirm the type changes for Month and Hour\n",
    "df_sql.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031aef55-aa16-4626-872e-f93994590346",
   "metadata": {},
   "source": [
    "### 1-3. Set up a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622479b3-199a-4f1b-8088-7b5050be2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the Hour column based on the column being less than 6.5 or not\n",
    "# 'Binarizer' converts values into 0 or 1 based on a specified threshold\n",
    "binarizer = Binarizer(threshold=6.5, inputCol=\"Hour\", outputCol=\"Hour_binary\")\n",
    "\n",
    "# One-hot encode the Month column\n",
    "# 'StringIndexer' converts categorical string values into numerical indices\n",
    "indexer = StringIndexer(inputCol=\"Month\", outputCol=\"Month_index\", handleInvalid=\"keep\")\n",
    "# 'OneHotEncoder' transforms the indexed values into one-hot encoded vectors\n",
    "encoder = OneHotEncoder(inputCol=\"Month_index\", outputCol=\"Month_OHE\")\n",
    "\n",
    "# Perform PCA\n",
    "# Reduce five variables into three principal components\n",
    "pca_input = [\"Temperature\", \"Humidity\", \"Wind_Speed\", \"General_Diffuse_Flows\", \"Diffuse_Flows\"]\n",
    "pca_assembler = VectorAssembler(inputCols=pca_input, outputCol=\"pca_input\")\n",
    "pca = PCA(k=3, inputCol=\"pca_input\", outputCol=\"pca_output\")\n",
    "\n",
    "# Final feature assembler\n",
    "features = [\"pca_output\", \"Hour_binary\", \"Power_Zone_1\", \"Power_Zone_2\", \"Month_OHE\"]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07fdafad-75ce-4039-b4ef-31d3e9014311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Transformer class to be used in the pipeline\n",
    "# This class renames a column in a Spark SQL DataFram \n",
    "class RenameColumn(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCol: str, outputCol: str):\n",
    "        super().__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        return dataset.withColumnRenamed(self.inputCol, self.outputCol)\n",
    "\n",
    "# Instantiate the Transformer to rename \"Power_Zone_3\" to \"response\"\n",
    "renamer = RenameColumn(inputCol=\"Power_Zone_3\", outputCol=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "307eb81b-a47e-446f-a76e-ccd56fecabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model - ElasticNet\n",
    "lr = LinearRegression(labelCol=\"response\", featuresCol=\"features\")\n",
    "\n",
    "# Parameter grid\n",
    "# 'ParamGridBuilder' generates a grid of parameter combinations for model selection\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]) \\\n",
    "    .build()\n",
    "\n",
    "# Define an evaluator to measure model performance\n",
    "# Here, the metric used is Root Mean Squared Error (RMSE)\n",
    "evaluator = RegressionEvaluator(labelCol=\"response\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4092d805-769e-46dd-a772-bf3e76158fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline combining all preprocessing and modeling stages\n",
    "pipeline = Pipeline(stages=[binarizer, indexer, encoder, pca_assembler, pca, assembler, renamer, lr])\n",
    "\n",
    "# Set up a CrossValidator to perform k-fold cross-validation for model selection\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1632eb6b-493b-454d-b8c1-33ed360f523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 14:02:23 WARN Instrumentation: [b49b0bd4] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:24 WARN Instrumentation: [b49b0bd4] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:26 WARN Instrumentation: [cfa8833e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:27 WARN Instrumentation: [cfa8833e] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:28 WARN Instrumentation: [2fa03562] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:28 WARN Instrumentation: [2fa03562] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:29 WARN Instrumentation: [6910c1ba] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:31 WARN Instrumentation: [7a147494] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:31 WARN Instrumentation: [7a147494] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:32 WARN Instrumentation: [08d6c3ce] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:32 WARN Instrumentation: [08d6c3ce] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:33 WARN Instrumentation: [1dd9aa43] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:35 WARN Instrumentation: [d87b1b39] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:35 WARN Instrumentation: [d87b1b39] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:36 WARN Instrumentation: [1d4b660f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:36 WARN Instrumentation: [1d4b660f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:38 WARN Instrumentation: [938e1d0c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:38 WARN Instrumentation: [938e1d0c] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:02:39 WARN Instrumentation: [41b35561] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:02:39 WARN Instrumentation: [41b35561] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:44 WARN Instrumentation: [221f1934] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:45 WARN Instrumentation: [1742ccef] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:45 WARN Instrumentation: [1742ccef] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:46 WARN Instrumentation: [7bfc2e5c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:46 WARN Instrumentation: [7bfc2e5c] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:47 WARN Instrumentation: [6b9af5c6] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:48 WARN Instrumentation: [bbffc6f2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:49 WARN Instrumentation: [bbffc6f2] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:50 WARN Instrumentation: [3948427a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:50 WARN Instrumentation: [3948427a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:51 WARN Instrumentation: [ae5998b1] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:51 WARN Instrumentation: [ae5998b1] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:52 WARN Instrumentation: [87ad083a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:53 WARN Instrumentation: [0531dd2d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:53 WARN Instrumentation: [0531dd2d] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:04:54 WARN Instrumentation: [95a23038] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:04:55 WARN Instrumentation: [e7ac975f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:06:58 WARN Instrumentation: [766367db] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:06:59 WARN Instrumentation: [d23c8898] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:00 WARN Instrumentation: [6fb99738] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:02 WARN Instrumentation: [315a5661] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:03 WARN Instrumentation: [80300492] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:04 WARN Instrumentation: [fe95c5a3] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:05 WARN Instrumentation: [97857123] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:05 WARN Instrumentation: [97857123] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:07:06 WARN Instrumentation: [a5d27eda] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:06 WARN Instrumentation: [a5d27eda] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:07:07 WARN Instrumentation: [d951e1c0] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:08 WARN Instrumentation: [db994406] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:09 WARN Instrumentation: [683edb8b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:07:09 WARN Instrumentation: [683edb8b] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:10 WARN Instrumentation: [9c6404c1] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:10 WARN Instrumentation: [9c6404c1] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:11 WARN Instrumentation: [4ca853cf] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:12 WARN Instrumentation: [eef1196f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:13 WARN Instrumentation: [eef1196f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:14 WARN Instrumentation: [4b33f0bb] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:14 WARN Instrumentation: [4b33f0bb] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:15 WARN Instrumentation: [fc56712c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:15 WARN Instrumentation: [fc56712c] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:16 WARN Instrumentation: [aa9fff30] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:16 WARN Instrumentation: [aa9fff30] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:17 WARN Instrumentation: [7290ec51] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:17 WARN Instrumentation: [7290ec51] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:18 WARN Instrumentation: [913b11c8] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:19 WARN Instrumentation: [6287535a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:20 WARN Instrumentation: [18e93e6f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:20 WARN Instrumentation: [18e93e6f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:09:21 WARN Instrumentation: [ced0af81] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:09:21 WARN Instrumentation: [ced0af81] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:11:22 WARN Instrumentation: [eab69743] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:24 WARN Instrumentation: [e0376701] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:25 WARN Instrumentation: [d1a58377] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:26 WARN Instrumentation: [1c9d6971] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:27 WARN Instrumentation: [2fd77ce1] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:28 WARN Instrumentation: [ae360fa0] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:29 WARN Instrumentation: [6e876a71] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:30 WARN Instrumentation: [a2a959ea] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:31 WARN Instrumentation: [c07b73ac] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:32 WARN Instrumentation: [c07b73ac] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:11:33 WARN Instrumentation: [8cd05682] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/28 14:11:33 WARN Instrumentation: [8cd05682] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/28 14:11:34 WARN Instrumentation: [061ace55] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 2137.4072\n",
      "+-----------+------------------+------------------+\n",
      "|   response|        prediction|          residual|\n",
      "+-----------+------------------+------------------+\n",
      "|20240.96386|20799.716959339567|-558.7530993395667|\n",
      "|20131.08434|18579.516457248305|1551.5678827516967|\n",
      "|19668.43373| 18129.45214722877|  1538.98158277123|\n",
      "|18899.27711|17515.275781662618| 1384.001328337381|\n",
      "|18442.40964| 16918.01521781402| 1524.394422185982|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline model using the training data\n",
    "cvModel = cv.fit(df_sql) \n",
    "\n",
    "# Make predictions on the training data\n",
    "predictions = cvModel.transform(df_sql)\n",
    "\n",
    "# Evaluate the model by calculating the training Root Mean Squared Error (RMSE)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Training RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Display the first five rows showing actual, predicted, and residual values\n",
    "predictions = predictions.withColumn(\"residual\", col(\"response\") - col(\"prediction\"))\n",
    "predictions.select(\"response\", \"prediction\", \"residual\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db5840-cafb-4de3-99dc-da24feb40f0e",
   "metadata": {},
   "source": [
    "## 2. Streaming Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6829b-3aaa-46ff-92e1-31acde39588e",
   "metadata": {},
   "source": [
    "### 2-1.  Reading a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd42efd-e4e7-4e46-8527-c2d373a072b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 14:13:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession for the streaming project\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"StreamingProject\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b1d224e-1db0-4224-a905-a3d469be6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the schema for the stream\n",
    "# Define the schema for the incoming streaming data\n",
    "schema = StructType() \\\n",
    "    .add(\"Temperature\", DoubleType()) \\\n",
    "    .add(\"Humidity\", DoubleType()) \\\n",
    "    .add(\"Wind_Speed\", DoubleType()) \\\n",
    "    .add(\"General_Diffuse_Flows\", DoubleType()) \\\n",
    "    .add(\"Diffuse_Flows\", DoubleType()) \\\n",
    "    .add(\"Power_Zone_1\", DoubleType()) \\\n",
    "    .add(\"Power_Zone_2\", DoubleType()) \\\n",
    "    .add(\"Power_Zone_3\", DoubleType()) \\\n",
    "    .add(\"Month\", DoubleType()) \\\n",
    "    .add(\"Hour\", DoubleType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaf5c2cb-3885-48bf-8079-c52aa7f4e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the readStream\n",
    "# Read streaming CSV files from the \"csv_file/\" directory\n",
    "# Use the predefined schema and specify that the files include a header\n",
    "streaming_df = spark.readStream.option(\"header\", True) \\\n",
    "                    .schema(schema) \\\n",
    "                    .csv(\"csv_file/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6621d1-4744-416b-95cc-4256aa6bc0f2",
   "metadata": {},
   "source": [
    "### 2-2.  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91805d73-86e5-4713-8cef-c6160bf58995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the full model to the streaming data\n",
    "predicted_stream = cvModel.transform(streaming_df)\n",
    "\n",
    "# Create a residual column and select response, prediction, and residual for output\n",
    "predicted_stream = predicted_stream.withColumn(\"residual\", col(\"response\") - col(\"prediction\")) \\\n",
    "                                   .select(\"response\", \"prediction\", \"residual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5212347-9df1-4901-a992-6cf11da6c5af",
   "metadata": {},
   "source": [
    "### 2-3.  Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe2ef30d-214e-4133-b029-024c793a77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing-only pipeline\n",
    "pipeline_stream = Pipeline(stages=[binarizer, indexer, encoder, pca_assembler, pca, assembler, renamer])\n",
    "\n",
    "# Fit the preprocessing pipeline on static data\n",
    "preprocessing_model = pipeline_stream.fit(df_sql)\n",
    "\n",
    "# Apply the fitted preprocessing pipeline to the streaming data\n",
    "transformed_stream = preprocessing_model.transform(streaming_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc8830-3a8e-421a-8b62-c9bd0af9ace9",
   "metadata": {},
   "source": [
    "### 2-4.  Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c36b53d-a50c-4d22-b658-986774d7a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the preprocessed streaming data with the predicted results on the \"response\" column\n",
    "final_df = transformed_stream.join(predicted_stream, on=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e8dc2-4822-46c6-b3d6-b6360e7ea4f3",
   "metadata": {},
   "source": [
    "### 2-5.  Writing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1684b20-575b-4a3a-afc8-b7cdb2366f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 14:21:22 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/28 14:21:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+-----------+--------------+--------------------+--------------------+--------------------+------------------+-------------------+\n",
      "|   response|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Month|Hour|Hour_binary|Month_index|     Month_OHE|           pca_input|          pca_output|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+-----------+--------------+--------------------+--------------------+--------------------+------------------+-------------------+\n",
      "|14232.28916|      3.541|    80.8|     0.085|                0.055|        0.115|  22584.3038| 13798.17629|  1.0| 5.0|        0.0|        3.0|(12,[3],[1.0])|[3.541,80.8,0.085...|[2.00443491580984...|(18,[0,1,2,4,5,9]...|13563.840389259556|  668.4487707404442|\n",
      "|25781.42259|       23.2|   52.09|     4.916|                0.051|         0.13| 29884.38538| 20187.34177|  7.0| 2.0|        0.0|        1.0|(12,[1],[1.0])|[23.2,52.09,4.916...|[1.05749211112852...|(18,[0,1,2,4,5,7]...|26495.770644937922| -714.3480549379237|\n",
      "|27907.93846|       21.8|   59.02|     0.077|                31.69|         26.9| 46715.76159| 25783.78378|  6.0|19.0|        1.0|        6.0|(12,[6],[1.0])|[21.8,59.02,0.077...|[-36.845277769375...|(18,[0,1,2,3,4,5,...|28727.051599419967| -819.1131394199656|\n",
      "|19055.54859|      21.79|   65.56|     4.911|                0.117|        0.067| 28320.53274| 18775.50158|  8.0| 4.0|        0.0|        4.0|(12,[4],[1.0])|[21.79,65.56,4.91...|[1.38053095129174...|(18,[0,1,2,4,5,10...|21210.672065052953|-2155.1234750529547|\n",
      "|15468.33856|      21.48|   53.45|     4.909|                212.8|         25.5| 30315.11654| 21729.25026|  8.0| 8.0|        1.0|        4.0|(12,[4],[1.0])|[21.48,53.45,4.90...|[-209.59666993077...|(18,[0,1,2,3,4,5,...|20543.995968499148| -5075.657408499148|\n",
      "|11805.95745|      21.35|    71.0|     4.919|                302.2|        240.3| 30545.64551| 22432.78008| 10.0|13.0|        1.0|        2.0|(12,[2],[1.0])|[21.35,71.0,4.919...|[-357.75404874109...|(18,[0,1,2,3,4,5,...| 10469.09338660563| 1336.8640633943705|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+-----------+--------------+--------------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+-----------+---------------+--------------------+--------------------+--------------------+------------------+-------------------+\n",
      "|   response|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Month|Hour|Hour_binary|Month_index|      Month_OHE|           pca_input|          pca_output|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+-----------+---------------+--------------------+--------------------+--------------------+------------------+-------------------+\n",
      "|19732.21757|      25.85|   47.35|     4.905|                233.2|        221.3| 29884.38538|  20244.3038|  7.0| 7.0|        1.0|        1.0| (12,[1],[1.0])|[25.85,47.35,4.90...|[-286.91089667589...|(18,[0,1,2,3,4,5,...|23603.696251177036|-3871.4786811770355|\n",
      "|21019.65829|      13.91|    66.5|     0.077|                0.033|        0.096| 34230.50847| 21432.21884|  2.0|23.0|        1.0|       11.0|(12,[11],[1.0])|[13.91,66.5,0.077...|[1.55671608107935...|(18,[0,1,2,3,4,5,...| 19626.42264739786| 1393.2356426021397|\n",
      "|8084.513806|      16.12|   65.59|     0.076|                31.92|        32.71| 27577.18631|  21904.8788| 12.0| 9.0|        1.0|       10.0|(12,[10],[1.0])|[16.12,65.59,0.07...|[-38.548270324675...|(18,[0,1,2,3,4,5,...| 9748.582118463346| -1664.068312463346|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+-----+----+-----------+-----------+---------------+--------------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9798:====>                                              (17 + 128) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Write the final streaming DataFrame to the console in append mode\n",
    "query = final_df.writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f191054f-a723-4146-983c-f37c68f03405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 14:21:57 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/04/28 14:21:57 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71400,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71402,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71405,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71401,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71404,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71403,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=12),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/12/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/12/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 12 (task 503268, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=138),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/138/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/138/left-keyToNumValues/.3.delta.5b219464-b4a8-4121-a749-22697816179d.TID503394.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 138 (task 503394, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 12 (task 503268, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 138 (task 503394, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/138/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/138/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 138.0 in stage 9798.0 (TID 503394) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 12.0 in stage 9798.0 (TID 503268) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=119),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/119/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/119/right-keyWithIndexToValue/..3.delta.1092565d-4818-4440-b543-9d5affac4866.TID503375.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 119 (task 503375, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 119 (task 503375, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 119.0 in stage 9798.0 (TID 503375) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71417,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=118),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/118/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/118/right-keyWithIndexToValue/..3.delta.772160f5-b5cf-47c6-a845-90a5d0109eb6.TID503374.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 118 (task 503374, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 118 (task 503374, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=104),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/104/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/104/right-keyWithIndexToValue/..3.delta.13456752-eb4a-45e7-9870-7f60e50e2e5e.TID503360.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 104 (task 503360, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 104 (task 503360, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 118.0 in stage 9798.0 (TID 503374) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=89),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/89/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/89/right-keyWithIndexToValue/..3.delta.d42e9324-42cc-4411-908f-6fc44fdb10ca.TID503345.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 89 (task 503345, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 89 (task 503345, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 104.0 in stage 9798.0 (TID 503360) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 89.0 in stage 9798.0 (TID 503345) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=30),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/30/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/30/right-keyWithIndexToValue/..3.delta.ac1074ff-cef5-4dde-9090-af6892cdde24.TID503286.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 30 (task 503286, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 30 (task 503286, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=37),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/37/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/37/right-keyWithIndexToValue/..3.delta.d3deb44c-f2b8-44ef-b772-4c795831374a.TID503293.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 37 (task 503293, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 37 (task 503293, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 30.0 in stage 9798.0 (TID 503286) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 37.0 in stage 9798.0 (TID 503293) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=56),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/56/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/56/right-keyWithIndexToValue/..3.delta.30cf8207-ecfa-446b-9b13-d70460a28e95.TID503312.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 56 (task 503312, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 56 (task 503312, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 56.0 in stage 9798.0 (TID 503312) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71427,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71428,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71429,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=83),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/83/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/83/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 83 (task 503339, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 83 (task 503339, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 83.0 in stage 9798.0 (TID 503339) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71433,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71434,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71438,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71432,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=27),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/27/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/27/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=47),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/47/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/47/right-keyWithIndexToValue/..3.delta.6ca22a6b-6715-46c3-8cc7-49d50cfaf8b7.TID503303.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 27 (task 503283, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 27 (task 503283, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 47 (task 503303, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 47 (task 503303, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=42),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/42/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/42/right-keyWithIndexToValue/..3.delta.7e06dbb7-1ce1-460b-906e-6402fae6c2ca.TID503298.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 42 (task 503298, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 42 (task 503298, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 47.0 in stage 9798.0 (TID 503303) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 27.0 in stage 9798.0 (TID 503283) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71435,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71437,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=32),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/32/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/32/right-keyWithIndexToValue/..3.delta.fea6cd58-b586-49c7-9474-cf2f5455c28f.TID503288.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 32 (task 503288, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 32 (task 503288, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=15),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/15/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/15/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 15 (task 503271, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 15 (task 503271, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 42.0 in stage 9798.0 (TID 503298) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 15.0 in stage 9798.0 (TID 503271) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 32.0 in stage 9798.0 (TID 503288) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=52),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/52/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/52/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 52 (task 503308, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 52 (task 503308, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 52.0 in stage 9798.0 (TID 503308) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=122),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/122/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/122/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 122 (task 503378, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 122 (task 503378, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 122.0 in stage 9798.0 (TID 503378) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71442,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=14),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/14/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/14/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71443,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=140),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/140/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/140/left-keyToNumValues/.3.delta.5fa0e0f3-932a-45ef-bf34-3fc98059e401.TID503396.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 14 (task 503270, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 140 (task 503396, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 140 (task 503396, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 14 (task 503270, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71441,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=62),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/62/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/62/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 62 (task 503318, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 62 (task 503318, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71474,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=90),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/90/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/90/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 90 (task 503346, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=112),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/112/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/112/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71440,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 90 (task 503346, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=66),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/66/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/66/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 112 (task 503368, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 112 (task 503368, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 66 (task 503322, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 66 (task 503322, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=77),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/77/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/77/right-keyWithIndexToValue/..3.delta.273f74f0-10eb-4070-8b72-9be2091917c1.TID503333.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 77 (task 503333, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 77 (task 503333, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=121),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/121/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/121/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=20),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/20/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/20/right-keyWithIndexToValue/..3.delta.cf69765c-35e7-491d-a8d3-6b5a12cc629d.TID503276.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 121 (task 503377, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 20 (task 503276, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 121 (task 503377, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 20 (task 503276, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=8),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/8/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/8/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=40),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/40/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/40/right-keyWithIndexToValue/..3.delta.c693a71f-3131-4e53-8632-ced97bc58694.TID503296.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71446,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 8 (task 503264, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 40 (task 503296, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 8 (task 503264, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=82),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/82/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/82/right-keyWithIndexToValue/..3.delta.ffaef40f-0ff1-4d1a-8d70-04b386b2ec6f.TID503338.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 40 (task 503296, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 82 (task 503338, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 82 (task 503338, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=132),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/132/left-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/132/left-keyWithIndexToValue/.3.delta.19b73445-530f-4fb2-adc0-d34c184e6421.TID503388.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=136),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/136/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/136/left-keyToNumValues/.3.delta.9300658f-6429-45da-8497-fcad41dcd7a1.TID503392.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=21),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/21/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/21/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=67),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/67/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/67/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=71),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/71/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/71/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 136 (task 503392, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=11),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/11/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/11/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=103),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/103/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/103/right-keyWithIndexToValue/..3.delta.fbd9c27a-9156-446f-a6fa-93fe305cf009.TID503359.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 136 (task 503392, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 71 (task 503327, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 103 (task 503359, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=143),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/143/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/143/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 67 (task 503323, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 132 (task 503388, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=0),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/0/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/0/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71439,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 143 (task 503399, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 67 (task 503323, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 132 (task 503388, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 11 (task 503267, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/140/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 143 (task 503399, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 11 (task 503267, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=9),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/9/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/9/right-keyWithIndexToValue/..3.delta.be50e776-44f7-405d-a5a0-8a61b0b031cb.TID503265.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 71 (task 503327, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 9 (task 503265, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=3),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/3/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/3/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 9 (task 503265, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=17),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/17/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/17/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 3 (task 503259, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=74),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/74/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/74/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 103 (task 503359, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 17 (task 503273, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=111),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/111/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/111/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 21 (task 503277, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 17 (task 503273, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=39),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/39/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/39/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=49),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/49/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/49/right-keyWithIndexToValue/..3.delta.c14f4ea5-cc82-4d45-9bb2-f9f5dd9e3c8d.TID503305.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 111 (task 503367, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 503256, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/136/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71473,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 74 (task 503330, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 3 (task 503259, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 503256, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/143/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 74 (task 503330, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=107),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/107/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/107/right-keyWithIndexToValue/..3.delta.250da88e-aa26-43cd-85fc-abc9875bb6b2.TID503363.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 107 (task 503363, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=137),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/137/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/137/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 107 (task 503363, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=125),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/125/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/125/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 137 (task 503393, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 125 (task 503381, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 125 (task 503381, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 49 (task 503305, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 14.0 in stage 9798.0 (TID 503270) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=1),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/1/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/1/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 49 (task 503305, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 137 (task 503393, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 39 (task 503295, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/140/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 21 (task 503277, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/132/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 503257, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 39 (task 503295, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/143/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=10),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/10/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/10/right-keyWithIndexToValue/..3.delta.55582223-eb44-4910-9770-305f42792776.TID503266.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=45),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/45/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/45/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 10 (task 503266, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 45 (task 503301, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 10 (task 503266, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=72),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/72/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/72/right-keyWithIndexToValue/..3.delta.6cb4f929-779b-47b3-bbe4-e0edecdb09a8.TID503328.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=86),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/86/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/86/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 45 (task 503301, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 86 (task 503342, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 20.0 in stage 9798.0 (TID 503276) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 111 (task 503367, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 86 (task 503342, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=4),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/4/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/4/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 503257, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 72 (task 503328, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 72 (task 503328, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 77.0 in stage 9798.0 (TID 503333) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 4 (task 503260, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 88.0 in stage 9798.0 (TID 503344) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 4 (task 503260, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 62.0 in stage 9798.0 (TID 503318) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 121.0 in stage 9798.0 (TID 503377) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 90.0 in stage 9798.0 (TID 503346) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 40.0 in stage 9798.0 (TID 503296) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/136/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/137/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 8.0 in stage 9798.0 (TID 503264) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 82.0 in stage 9798.0 (TID 503338) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=94),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/94/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/94/right-keyWithIndexToValue/..3.delta.3dde4804-f065-44c7-a285-39a85753e2fd.TID503350.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 112.0 in stage 9798.0 (TID 503368) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=93),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/93/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/93/right-keyWithIndexToValue/..3.delta.94f51604-867f-431a-afb6-a20080484980.TID503349.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 94 (task 503350, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 93 (task 503349, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 94 (task 503350, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 93 (task 503349, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=80),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/80/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/80/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 9.0 in stage 9798.0 (TID 503265) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 80 (task 503336, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 80 (task 503336, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/137/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 19.0 in stage 9798.0 (TID 503275) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=144),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/144/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.InterruptedIOException: java.lang.InterruptedException\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1010)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "Caused by: java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:328)\n",
      "\tat java.base/java.lang.ProcessImpl.waitFor(ProcessImpl.java:495)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1000)\n",
      "\t... 61 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 66.0 in stage 9798.0 (TID 503322) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 144 (task 503400, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=64),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/64/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/64/right-keyWithIndexToValue/..3.delta.4a36d114-c053-4356-8b04-73dcb12a576f.TID503320.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 67.0 in stage 9798.0 (TID 503323) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 144 (task 503400, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 64 (task 503320, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 11.0 in stage 9798.0 (TID 503267) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 71.0 in stage 9798.0 (TID 503327) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 64 (task 503320, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 103.0 in stage 9798.0 (TID 503359) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 0.0 in stage 9798.0 (TID 503256) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 17.0 in stage 9798.0 (TID 503273) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=22),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/22/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/22/right-keyWithIndexToValue/..3.delta.9b48476f-debf-4f45-aae5-8ed96b5e2e48.TID503278.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 21.0 in stage 9798.0 (TID 503277) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71539,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 22 (task 503278, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 107.0 in stage 9798.0 (TID 503363) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=59),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/59/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/59/right-keyWithIndexToValue/..3.delta.ff3c7b17-79c1-4d63-928d-e5ecde7f147a.TID503315.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/144/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 22 (task 503278, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 59 (task 503315, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 59 (task 503315, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 125.0 in stage 9798.0 (TID 503381) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71537,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/144/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 140.0 in stage 9798.0 (TID 503396) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=105),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/105/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/105/right-keyWithIndexToValue/..3.delta.830f5b8b-e5d2-4191-84b1-18f18949b01e.TID503361.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 105 (task 503361, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 105 (task 503361, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=126),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/126/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/126/right-keyWithIndexToValue/..3.delta.c519eb28-dc02-4543-b49f-655fc43f41de.TID503382.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 10.0 in stage 9798.0 (TID 503266) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 126 (task 503382, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 126 (task 503382, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 143.0 in stage 9798.0 (TID 503399) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 49.0 in stage 9798.0 (TID 503305) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 1.0 in stage 9798.0 (TID 503257) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 74.0 in stage 9798.0 (TID 503330) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=123),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/123/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/123/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71532,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71529,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71526,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=128),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/128/right-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.InterruptedIOException: java.lang.InterruptedException\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1010)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "Caused by: java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:328)\n",
      "\tat java.base/java.lang.ProcessImpl.waitFor(ProcessImpl.java:495)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1000)\n",
      "\t... 61 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 128 (task 503384, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 123 (task 503379, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 128 (task 503384, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 123 (task 503379, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71524,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71530,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 39.0 in stage 9798.0 (TID 503295) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 45.0 in stage 9798.0 (TID 503301) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 4.0 in stage 9798.0 (TID 503260) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=135),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/135/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/135/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 135 (task 503391, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 135 (task 503391, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 86.0 in stage 9798.0 (TID 503342) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=38),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/38/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/38/right-keyWithIndexToValue/..3.delta.5332e782-8dc7-4eb1-b5d8-5baf1cfdac58.TID503294.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=50),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/50/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/50/right-keyWithIndexToValue/..3.delta.cecb499a-855f-4a9f-8bb1-fce343c06b1a.TID503306.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 38 (task 503294, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 111.0 in stage 9798.0 (TID 503367) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 38 (task 503294, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 50 (task 503306, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=33),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/33/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/33/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 72.0 in stage 9798.0 (TID 503328) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 50 (task 503306, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=31),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/31/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/31/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=127),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/127/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/127/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 33 (task 503289, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 33 (task 503289, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=70),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/70/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/70/right-keyWithIndexToValue/..3.delta.99b845ec-df31-4c3d-b0ed-7d0eaef7ab39.TID503326.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 127 (task 503383, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 31 (task 503287, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=25),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/25/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/25/right-keyWithIndexToValue/..3.delta.d443ab26-f0a5-4ae4-9707-8ab201fd7015.TID503281.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 127 (task 503383, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=75),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/75/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/75/right-keyWithIndexToValue/..3.delta.6f63fb98-0355-4fa8-9972-470b54e5a39e.TID503331.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 31 (task 503287, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 70 (task 503326, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71506,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71510,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/135/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 70 (task 503326, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71507,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=85),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/85/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/85/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=23),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/23/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/23/right-keyWithIndexToValue/..3.delta.7c874b3c-ecc1-4ba8-b239-8e8abf2263fa.TID503279.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71502,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71500,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 23 (task 503279, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=97),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/97/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/97/right-keyWithIndexToValue/..3.delta.8e20c219-1203-4ccb-9bb0-c9104087ff32.TID503353.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71513,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 75 (task 503331, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71512,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=84),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/84/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/84/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 25 (task 503281, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 136.0 in stage 9798.0 (TID 503392) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=28),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/28/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/28/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=108),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/108/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/108/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=35),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/35/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/35/right-keyWithIndexToValue/..3.delta.ca28aca5-8a78-45a3-8cf5-61c0aede3194.TID503291.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71556,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 97 (task 503353, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 93.0 in stage 9798.0 (TID 503349) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 108 (task 503364, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 28 (task 503284, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 23 (task 503279, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 85 (task 503341, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 85 (task 503341, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=18),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/18/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/18/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 35 (task 503291, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 97 (task 503353, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 35 (task 503291, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 75 (task 503331, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 108 (task 503364, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 18 (task 503274, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 84 (task 503340, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 18 (task 503274, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 25 (task 503281, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 28 (task 503284, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 84 (task 503340, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=96),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/96/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/96/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=26),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/26/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/26/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 96 (task 503352, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 96 (task 503352, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 26 (task 503282, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 26 (task 503282, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/135/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=102),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/102/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/102/right-keyWithIndexToValue/..3.delta.b959430f-c45c-44ae-84d0-558cb0751028.TID503358.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=95),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/95/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/95/right-keyWithIndexToValue/..3.delta.5621ca43-7917-4cea-a388-abe6c21a7528.TID503351.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 95 (task 503351, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 102 (task 503358, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 95 (task 503351, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 102 (task 503358, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=92),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/92/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/92/right-keyWithIndexToValue/..3.delta.855384fb-00f0-49bb-a4d3-723a9e3efc9d.TID503348.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 92 (task 503348, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 92 (task 503348, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 3.0 in stage 9798.0 (TID 503259) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 132.0 in stage 9798.0 (TID 503388) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 137.0 in stage 9798.0 (TID 503393) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 94.0 in stage 9798.0 (TID 503350) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN Shell: Interrupted while joining on: Thread[Thread-71560,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 64.0 in stage 9798.0 (TID 503320) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 80.0 in stage 9798.0 (TID 503336) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=36),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/36/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/36/right-keyWithIndexToValue/..3.delta.8c0c95e6-86c5-4b15-84c2-6c071065b8b9.TID503292.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 36 (task 503292, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 36 (task 503292, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 59.0 in stage 9798.0 (TID 503315) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 22.0 in stage 9798.0 (TID 503278) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 105.0 in stage 9798.0 (TID 503361) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 144.0 in stage 9798.0 (TID 503400) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 126.0 in stage 9798.0 (TID 503382) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 58.0 in stage 9798.0 (TID 503314) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 16.0 in stage 9798.0 (TID 503272) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 123.0 in stage 9798.0 (TID 503379) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 38.0 in stage 9798.0 (TID 503294) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 31.0 in stage 9798.0 (TID 503287) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 29.0 in stage 9798.0 (TID 503285) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 50.0 in stage 9798.0 (TID 503306) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 63.0 in stage 9798.0 (TID 503319) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 127.0 in stage 9798.0 (TID 503383) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 33.0 in stage 9798.0 (TID 503289) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 70.0 in stage 9798.0 (TID 503326) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 113.0 in stage 9798.0 (TID 503369) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 43.0 in stage 9798.0 (TID 503299) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 5.0 in stage 9798.0 (TID 503261) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 60.0 in stage 9798.0 (TID 503316) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 44.0 in stage 9798.0 (TID 503300) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 75.0 in stage 9798.0 (TID 503331) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 85.0 in stage 9798.0 (TID 503341) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 23.0 in stage 9798.0 (TID 503279) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 18.0 in stage 9798.0 (TID 503274) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 28.0 in stage 9798.0 (TID 503284) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 84.0 in stage 9798.0 (TID 503340) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 35.0 in stage 9798.0 (TID 503291) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 108.0 in stage 9798.0 (TID 503364) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 25.0 in stage 9798.0 (TID 503281) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=41),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/41/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/41/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 26.0 in stage 9798.0 (TID 503282) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 41 (task 503297, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 41 (task 503297, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 97.0 in stage 9798.0 (TID 503353) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 96.0 in stage 9798.0 (TID 503352) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 135.0 in stage 9798.0 (TID 503391) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 95.0 in stage 9798.0 (TID 503351) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 102.0 in stage 9798.0 (TID 503358) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 79.0 in stage 9798.0 (TID 503335) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=55),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/55/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/55/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 36.0 in stage 9798.0 (TID 503292) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 55 (task 503311, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 55 (task 503311, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 98.0 in stage 9798.0 (TID 503354) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=54),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/54/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/54/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 92.0 in stage 9798.0 (TID 503348) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 54 (task 503310, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 54 (task 503310, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 41.0 in stage 9798.0 (TID 503297) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 46.0 in stage 9798.0 (TID 503302) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 55.0 in stage 9798.0 (TID 503311) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 54.0 in stage 9798.0 (TID 503310) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 24.0 in stage 9798.0 (TID 503280) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=133),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/133/left-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/133/left-keyWithIndexToValue/.3.delta.a67d2d76-d3cf-4c0d-ae09-2ba4b0e11644.TID503389.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 133 (task 503389, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 133 (task 503389, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/133/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=139),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/139/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/139/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 139 (task 503395, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 139 (task 503395, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 69.0 in stage 9798.0 (TID 503325) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=109),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/109/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/109/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=110),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/110/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/110/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 109 (task 503365, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 109 (task 503365, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 110 (task 503366, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/139/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 110 (task 503366, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=116),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/116/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/116/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 133.0 in stage 9798.0 (TID 503389) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 116 (task 503372, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 87.0 in stage 9798.0 (TID 503343) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 116 (task 503372, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=129),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/129/left-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/129/left-keyWithIndexToValue/..3.delta.7cb30da9-e691-4cb9-a6b6-58a07603f68f.TID503385.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/139/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 129 (task 503385, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 129 (task 503385, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/129/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=61),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/61/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/61/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 110.0 in stage 9798.0 (TID 503366) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 116.0 in stage 9798.0 (TID 503372) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 61 (task 503317, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 61 (task 503317, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 139.0 in stage 9798.0 (TID 503395) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 129.0 in stage 9798.0 (TID 503385) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 109.0 in stage 9798.0 (TID 503365) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 61.0 in stage 9798.0 (TID 503317) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=130),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/130/left-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/130/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:784)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 130 (task 503386, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 130 (task 503386, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/130/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 130.0 in stage 9798.0 (TID 503386) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 124.0 in stage 9798.0 (TID 503380) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=114),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/114/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/114/right-keyWithIndexToValue/..3.delta.d5a1f4bc-aa91-4bd0-be4f-8b5e223c8de7.TID503370.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\n",
      "\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1624)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 114 (task 503370, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 114 (task 503370, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=142),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/142/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/142/left-keyToNumValues/.3.delta.7bd593cb-5015-45ac-9344-3d03ded7028b.TID503398.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 114.0 in stage 9798.0 (TID 503370) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 142 (task 503398, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 142 (task 503398, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/142/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 101.0 in stage 9798.0 (TID 503357) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/142/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=34),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/34/right-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/34/right-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 34 (task 503290, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 34 (task 503290, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 142.0 in stage 9798.0 (TID 503398) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 100.0 in stage 9798.0 (TID 503356) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 91.0 in stage 9798.0 (TID 503347) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 34.0 in stage 9798.0 (TID 503290) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 81.0 in stage 9798.0 (TID 503337) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 120.0 in stage 9798.0 (TID 503376) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=141),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/141/left-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/141/left-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 141 (task 503397, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 141 (task 503397, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/141/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/141/left-keyWithIndexToValue does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 141.0 in stage 9798.0 (TID 503397) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=134),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/134/left-keyWithIndexToValue]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/134/left-keyWithIndexToValue/.3.delta.daa2a394-cd96-4d6c-8c6b-cdcd1319b4e9.TID503390.tmp does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 134 (task 503390, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 134 (task 503390, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/134/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 128.0 in stage 9798.0 (TID 503384) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 134.0 in stage 9798.0 (TID 503390) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n",
      "25/04/28 14:21:57 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 3 into HDFSStateStore[id=(op=0,part=131),dir=file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/131/right-keyToNumValues]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:704)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:470)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/131/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 33 more\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborting commit for partition 131 (task 503387, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR DataWritingSparkTask: Aborted commit for partition 131 (task 503387, attempt 0, stage 9798.0)\n",
      "25/04/28 14:21:57 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-3c10979f-c9e4-46ab-866b-929379a5e9bb/state/0/131/right-keyToNumValues does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/28 14:21:57 WARN TaskSetManager: Lost task 131.0 in stage 9798.0 (TID 503387) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job 6121 cancelled part of cancelled job group 367b5dc1-98e9-4d12-9e33-25ffc956bfcc)\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36e06d-41f1-458c-84d2-05a003a92b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
