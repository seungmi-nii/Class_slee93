{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc98557-a3f7-46d9-a07b-44e29d911a51",
   "metadata": {},
   "source": [
    "## Data load & Data preprocessing\n",
    "\n",
    "* **The Energy Efficiency Dataset** from the UCI Machine Learning Repository contains information related to the thermal performance of buildings.\n",
    "* It includes **8 predictor variables** describing physical and design parameters such as surface area, wall and roof area, building orientation, and glazing area.\n",
    "* The dataset provides two response variables: Heating Load and Cooling Load, representing the building’s energy demand for heating and cooling.\n",
    "* In this project, a predictive model (regression) was developed to estimate the **Heating Load** based on the given building features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03b8589-c074-483f-871e-c6572f702b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PolynomialExpansion, Interaction\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.ml.feature import VectorAssembler as VA2\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1bc437-7cb2-4de7-8b4c-58753c109e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/08 15:03:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"EnergyEfficiencyRegression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8f973b-4e67-4b02-a1cb-8d000f0357d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv(\"ENB2012_data.csv\")\n",
    "\n",
    "# Rename columns\n",
    "df_pandas.columns = [\"Relative_Compactness\",        # X1\n",
    "                     \"Surface_Area\",                # X2\n",
    "                     \"Wall_Area\",                   # X3\n",
    "                     \"Roof_Area\",                   # X4\n",
    "                     \"Overall_Height\",              # X5\n",
    "                     \"Orientation\",                 # X6\n",
    "                     \"Glazing_Area\",                # X7\n",
    "                     \"Glazing_Area_Distribution\",  # X8\n",
    "                     \"Heating_Load\",                # Y1\n",
    "                     \"Cooling_Load\"                 # Y2\n",
    "                    ]\n",
    "\n",
    "# Pandas -> PySpark DataFrame\n",
    "df = spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb64b59-f7fe-400b-bf40-a5603ed537ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Relative_Compactness: double (nullable = true)\n",
      " |-- Surface_Area: double (nullable = true)\n",
      " |-- Wall_Area: double (nullable = true)\n",
      " |-- Roof_Area: double (nullable = true)\n",
      " |-- Overall_Height: double (nullable = true)\n",
      " |-- Orientation: double (nullable = true)\n",
      " |-- Glazing_Area: double (nullable = true)\n",
      " |-- Glazing_Area_Distribution: double (nullable = true)\n",
      " |-- Heating_Load: double (nullable = true)\n",
      " |-- Cooling_Load: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:==============>                                        (34 + 94) / 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# heck dataset information\n",
    "df.printSchema()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14087d-0de2-4406-8262-045b46dd3b03",
   "metadata": {},
   "source": [
    "* For simplicity, missing values in the dataset were removed prior to model training.\n",
    "* Alternatively, if desired, various imputation methods (e.g., mean, median, or interpolation) can be applied to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e902b0-0c78-4fce-a113-ec1e89d4938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516d813-b8ca-40be-8fd5-4fe3ca8b7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c28cb-890b-4d24-bc47-06fa39f2d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for transformation\n",
    "feature_cols = [\"Surface_Area\", \"Wall_Area\", \"Roof_Area\", \"Overall_Height\", \"Glazing_Area\",\n",
    "                \"Orientation\", \"Glazing_Area_Distribution\"]\n",
    "feature_poly = [\"Surface_Area\", \"Wall_Area\", \"Roof_Area\", \"Glazing_Area\"]\n",
    "feature_inter = [\"Orientation\", \"Glazing_Area\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7b6c-44d1-451a-9d02-474597f9398c",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474a23f-423a-4f6f-933a-ceec42907b49",
   "metadata": {},
   "source": [
    "Since the response value in this project (Heating Load) is a continuous numerical value, a regression approach was required to predict it effectively.  \n",
    "To address this, I selected and compared three different regression models: Multiple Linear Regression, Regression Tree, and Random Forests.  \n",
    "* **Multiple Linear Regression model**\n",
    "  * A statistical method that models the linear relationship between multiple predictor variables and a continuous response value.\n",
    "  * It assumes that the response value is a linear combination of the predictor variables.  \n",
    "\n",
    "* **Regression Tree model**\n",
    "  - A decision tree-based model that splits the data into regions using feature thresholds to minimize prediction error.\n",
    "  - It captures non-linear relationships and is easy to interpret.  \n",
    "\n",
    "* **Random Forests model**\n",
    "  - An ensemble method based on bagging (bootstrap aggregating)\n",
    "  - It constructs multiple decision trees using random subsets of the training data (with replacement).\n",
    "  - For each tree, a random subset of predictor variables is selected at each split, enhancing model diversity.\n",
    "  - It improves accuracy and reduces overfitting compared to a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b0298-f64c-4a10-abfb-aa1b0e66b2c0",
   "metadata": {},
   "source": [
    "#### Metric\n",
    "* Metric used: Root Mean Squared Error (RMSE)\n",
    "* RMSE measures the square root of the average squared differences between predicted and actual values.\n",
    "* It penalizes large errors more than small ones, making it sensitive to outliers.\n",
    "* RMSE is widely used in regression tasks and provides results in the same unit as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e918be-d792-46aa-ab1d-786932db5e9a",
   "metadata": {},
   "source": [
    "In this section, pipelines were constructed for each regression model, and cross-validation was performed to identify the best-performing model.  \n",
    "The best model was then retrained using the entire dataset to finalize the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396031c-fdb3-49d6-b7e0-fa047c7a4ede",
   "metadata": {},
   "source": [
    "### 1. Multiple Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64e48075-36a2-48df-a154-4e69099f4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# Feature vectorization\n",
    "assembler_lr = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "# Vectorization → Normalization\n",
    "scaler_lr = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49fd27d7-4fc3-44b6-a316-b4c77939a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Heating_Load\")\n",
    "\n",
    "# Create parameter grid\n",
    "paramGrid_lr = ParamGridBuilder()  \\\n",
    "    .addGrid(lr.regParam, [0, 0.01, 0.04, 0.06, 0.1])  \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.5, 0.8, 1])  \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b05f9e-6f94-405d-bafe-738181abf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipeline_lr = Pipeline(stages = [assembler_lr, scaler_lr, lr]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f38f07c8-a7a1-4731-a3af-41e98c6d75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation object\n",
    "crossval_lr = CrossValidator(estimator = pipeline_lr, \n",
    "                             estimatorParamMaps = paramGrid_lr,\n",
    "                             evaluator = RegressionEvaluator(labelCol=\"Heating_Load\", metricName='rmse'),\n",
    "                             numFolds = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8110a171-a0f8-422b-9d61-5ead1ddd43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 15:03:46 WARN Instrumentation: [db68090b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:03:46 WARN Instrumentation: [db68090b] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:03:50 WARN Instrumentation: [da1d07fd] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:03:50 WARN Instrumentation: [da1d07fd] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:03:51 WARN Instrumentation: [207ea4a8] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:03:52 WARN Instrumentation: [a15caecc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:04 WARN Instrumentation: [18df9b33] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:04 WARN Instrumentation: [18df9b33] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:04 WARN Instrumentation: [6f46ee87] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:05 WARN Instrumentation: [6f46ee87] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:05 WARN Instrumentation: [dece0501] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:05 WARN Instrumentation: [dece0501] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:06 WARN Instrumentation: [b6991080] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:06 WARN Instrumentation: [b6991080] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:16 WARN Instrumentation: [024bc5ee] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:16 WARN Instrumentation: [024bc5ee] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:17 WARN Instrumentation: [a68f9434] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:17 WARN Instrumentation: [a68f9434] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:17 WARN Instrumentation: [662f7fac] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:18 WARN Instrumentation: [662f7fac] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:18 WARN Instrumentation: [89bef95b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:18 WARN Instrumentation: [89bef95b] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:28 WARN Instrumentation: [1d04b54d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:29 WARN Instrumentation: [2a885fa2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:29 WARN Instrumentation: [b44859e6] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:30 WARN Instrumentation: [c7039d10] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:40 WARN Instrumentation: [b8906e48] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:40 WARN Instrumentation: [b8906e48] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:41 WARN Instrumentation: [238a5038] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:41 WARN Instrumentation: [238a5038] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:41 WARN Instrumentation: [7ddced3e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:41 WARN Instrumentation: [7ddced3e] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:42 WARN Instrumentation: [71ef431c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/04/08 15:04:42 WARN Instrumentation: [71ef431c] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/04/08 15:04:52 WARN Instrumentation: [6c00e28b] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "lr_cv = crossval_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c81be88-cf5b-48e3-91c1-131229c59eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.9103895221806093, dict_values([0.0, 0.0])],\n",
       " [2.910389522180748, dict_values([0.0, 0.5])],\n",
       " [2.9103895221605622, dict_values([0.0, 0.8])],\n",
       " [2.910389522160519, dict_values([0.0, 1.0])],\n",
       " [2.9105541822095935, dict_values([0.01, 0.0])],\n",
       " [2.9109813782581986, dict_values([0.01, 0.5])],\n",
       " [2.9111042830531053, dict_values([0.01, 0.8])],\n",
       " [2.911354879179608, dict_values([0.01, 1.0])],\n",
       " [2.913938756007699, dict_values([0.04, 0.0])],\n",
       " [2.9165763914472778, dict_values([0.04, 0.5])],\n",
       " [2.9177139469363405, dict_values([0.04, 0.8])],\n",
       " [2.9176441309951087, dict_values([0.04, 1.0])],\n",
       " [2.917919115974555, dict_values([0.06, 0.0])],\n",
       " [2.943560851440754, dict_values([0.06, 0.5])],\n",
       " [2.917737956079703, dict_values([0.06, 0.8])],\n",
       " [2.9291954360425416, dict_values([0.06, 1.0])],\n",
       " [2.9283364363744133, dict_values([0.1, 0.0])],\n",
       " [2.9177106317165924, dict_values([0.1, 0.5])],\n",
       " [2.9186097108722544, dict_values([0.1, 0.8])],\n",
       " [2.9196404139598595, dict_values([0.1, 1.0])]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_list = []\n",
    "for i in range(len(paramGrid_lr)):\n",
    "    lr_list.append([lr_cv.avgMetrics[i], paramGrid_lr[i].values()])\n",
    "lr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488b5a20-f153-4eca-8fb9-dbf82cd2e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model parameters\n",
    "best_index_lr = lr_cv.avgMetrics.index(min(lr_cv.avgMetrics))\n",
    "best_params_lr = paramGrid_lr[best_index_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9d5d97-9a2e-48bb-a736-a58c57f495fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 15:07:37 WARN Instrumentation: [a324b2c2] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Train best model on the full dataset\n",
    "lr_final = pipeline_lr.copy(best_params_lr).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297e1a1-76d8-49dc-8bd3-03392ab65ba8",
   "metadata": {},
   "source": [
    "### 2. Regression Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d4e470e-48c5-4a9a-bdf0-7bdf2b1898e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# Feature vectorization\n",
    "assembler_dt = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdf507-91c3-4e2e-925e-811cfec3879a",
   "metadata": {},
   "source": [
    "Note that normalization was not applied to the tree-based models are not sensitive to feature scaling.  \n",
    "These models split data based on feature thresholds and do not rely on distance-based calculations, making normalization unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477eb139-1e51-4e33-8478-4d88647c9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tree model ojbect\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Heating_Load\")\n",
    "\n",
    "# Create parameter grid\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 10]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea039e70-4b90-444b-87b9-463a7135d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipeline_dt = Pipeline(stages=[assembler_dt, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ef115b0-2a51-4c95-be10-513cb19a8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validation object\n",
    "crossval_dt = CrossValidator(\n",
    "    estimator=pipeline_dt,\n",
    "    estimatorParamMaps=paramGrid_dt,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"Heating_Load\", metricName=\"rmse\"),\n",
    "    numFolds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8de3d062-9a6d-46b8-820f-7de3b8b91518",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cv = crossval_dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "251b7780-79bf-4537-946e-be363f4a0238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.434899777059463, dict_values([3, 1])],\n",
       " [2.434899777059463, dict_values([3, 2])],\n",
       " [2.434899777059463, dict_values([3, 4])],\n",
       " [1.136697260472595, dict_values([5, 1])],\n",
       " [1.1601286674276732, dict_values([5, 2])],\n",
       " [1.2689179779053141, dict_values([5, 4])],\n",
       " [0.6482622977146174, dict_values([10, 1])],\n",
       " [0.6971580927109831, dict_values([10, 2])],\n",
       " [0.8099170243838809, dict_values([10, 4])]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_list = []\n",
    "for i in range(len(paramGrid_dt)):\n",
    "    dt_list.append([dt_cv.avgMetrics[i], paramGrid_dt[i].values()])\n",
    "dt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cab154e-121e-467f-abe0-5b49ffbc7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model parameters\n",
    "best_index_dt = dt_cv.avgMetrics.index(min(dt_cv.avgMetrics))\n",
    "best_params_dt = paramGrid_dt[best_index_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a830e6c2-01cb-4d8e-bb19-5b808e9e9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on the full dataset\n",
    "dt_final = pipeline_dt.copy(best_params_dt).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c968c6-643d-4a9c-8c60-725b141ce8c6",
   "metadata": {},
   "source": [
    "### 3. Random Forests model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d346371-cd73-42ef-8954-830cb83829fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# Feature vectorization -1\n",
    "assembler_base = VectorAssembler(inputCols=feature_cols, outputCol=\"base_features\")\n",
    "\n",
    "# Feature vectorization -2\n",
    "assembler_poly = VectorAssembler(inputCols=feature_poly, outputCol=\"poly_input\")\n",
    "# Vectorization → Add polynomial features\n",
    "poly = PolynomialExpansion(inputCol=\"poly_input\", outputCol=\"poly_features\", degree=2)\n",
    "\n",
    "# Feature vectorization -3\n",
    "assembler_f1 = VectorAssembler(inputCols=[\"Wall_Area\"], outputCol=\"vec1\")\n",
    "assembler_f2 = VectorAssembler(inputCols=[\"Glazing_Area\"], outputCol=\"vec2\")\n",
    "# Vectorization → Add interaction features\n",
    "interaction = Interaction(inputCols=[\"vec1\", \"vec2\"], outputCol=\"interaction_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd93f0c1-b182-4a23-a49c-e7c400d03a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features (polynomial + interaction) → Vectorization\n",
    "final_features = VectorAssembler(inputCols=[\"base_features\", \"poly_features\", \"interaction_feature\"],\n",
    "                                 outputCol=\"assembled_features\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a76dfe2e-631b-48ee-ae51-c2b596e95790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization → Normalization\n",
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b0d5778-44a5-4996-a219-6484e810ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model ojbect\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Heating_Load\")\n",
    "\n",
    "# Create parameter grid\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 30, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "019ad92f-828b-42ff-bcec-cff5b2a8923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipeline_rf = Pipeline(stages = [assembler_base, assembler_poly, poly, assembler_f1, assembler_f2, interaction, final_features, scaler, rf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0213d68-911c-437c-ba57-1cf3b7a2fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGrid_rf,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"Heating_Load\", metricName=\"rmse\"),\n",
    "    numFolds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df87247a-b636-43fe-a614-34ccbdac7f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/jars/spark-core_2.12-3.5.5.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/04/08 15:10:00 WARN DAGScheduler: Broadcasting large task binary with size 1100.5 KiB\n",
      "25/04/08 15:10:09 WARN DAGScheduler: Broadcasting large task binary with size 1100.5 KiB\n",
      "25/04/08 15:10:10 WARN DAGScheduler: Broadcasting large task binary with size 1162.4 KiB\n",
      "25/04/08 15:10:11 WARN DAGScheduler: Broadcasting large task binary with size 1043.8 KiB\n",
      "25/04/08 15:10:22 WARN DAGScheduler: Broadcasting large task binary with size 1265.5 KiB\n",
      "25/04/08 15:10:24 WARN DAGScheduler: Broadcasting large task binary with size 1526.9 KiB\n",
      "25/04/08 15:10:26 WARN DAGScheduler: Broadcasting large task binary with size 1703.0 KiB\n",
      "25/04/08 15:10:33 WARN DAGScheduler: Broadcasting large task binary with size 1265.5 KiB\n",
      "25/04/08 15:10:35 WARN DAGScheduler: Broadcasting large task binary with size 1526.9 KiB\n",
      "25/04/08 15:10:36 WARN DAGScheduler: Broadcasting large task binary with size 1703.0 KiB\n",
      "25/04/08 15:10:38 WARN DAGScheduler: Broadcasting large task binary with size 1799.2 KiB\n",
      "25/04/08 15:10:39 WARN DAGScheduler: Broadcasting large task binary with size 1683.0 KiB\n",
      "25/04/08 15:10:40 WARN DAGScheduler: Broadcasting large task binary with size 1124.5 KiB\n",
      "25/04/08 15:11:05 WARN DAGScheduler: Broadcasting large task binary with size 1069.9 KiB\n",
      "25/04/08 15:11:13 WARN DAGScheduler: Broadcasting large task binary with size 1069.9 KiB\n",
      "25/04/08 15:11:14 WARN DAGScheduler: Broadcasting large task binary with size 1144.9 KiB\n",
      "25/04/08 15:11:15 WARN DAGScheduler: Broadcasting large task binary with size 1143.0 KiB\n",
      "25/04/08 15:11:26 WARN DAGScheduler: Broadcasting large task binary with size 1255.8 KiB\n",
      "25/04/08 15:11:28 WARN DAGScheduler: Broadcasting large task binary with size 1518.4 KiB\n",
      "25/04/08 15:11:30 WARN DAGScheduler: Broadcasting large task binary with size 1700.6 KiB\n",
      "25/04/08 15:11:37 WARN DAGScheduler: Broadcasting large task binary with size 1255.8 KiB\n",
      "25/04/08 15:11:39 WARN DAGScheduler: Broadcasting large task binary with size 1518.4 KiB\n",
      "25/04/08 15:11:41 WARN DAGScheduler: Broadcasting large task binary with size 1700.6 KiB\n",
      "25/04/08 15:11:42 WARN DAGScheduler: Broadcasting large task binary with size 1784.4 KiB\n",
      "25/04/08 15:11:44 WARN DAGScheduler: Broadcasting large task binary with size 1756.9 KiB\n",
      "25/04/08 15:12:10 WARN DAGScheduler: Broadcasting large task binary with size 1096.8 KiB\n",
      "25/04/08 15:12:18 WARN DAGScheduler: Broadcasting large task binary with size 1096.8 KiB\n",
      "25/04/08 15:12:19 WARN DAGScheduler: Broadcasting large task binary with size 1166.3 KiB\n",
      "25/04/08 15:12:20 WARN DAGScheduler: Broadcasting large task binary with size 1163.4 KiB\n",
      "25/04/08 15:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1265.7 KiB\n",
      "25/04/08 15:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1532.5 KiB\n",
      "25/04/08 15:12:35 WARN DAGScheduler: Broadcasting large task binary with size 1715.7 KiB\n",
      "25/04/08 15:12:43 WARN DAGScheduler: Broadcasting large task binary with size 1265.7 KiB\n",
      "25/04/08 15:12:45 WARN DAGScheduler: Broadcasting large task binary with size 1532.5 KiB\n",
      "25/04/08 15:12:46 WARN DAGScheduler: Broadcasting large task binary with size 1715.7 KiB\n",
      "25/04/08 15:12:48 WARN DAGScheduler: Broadcasting large task binary with size 1809.6 KiB\n",
      "25/04/08 15:12:49 WARN DAGScheduler: Broadcasting large task binary with size 1748.7 KiB\n",
      "25/04/08 15:12:50 WARN DAGScheduler: Broadcasting large task binary with size 1097.1 KiB\n",
      "25/04/08 15:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1104.8 KiB\n",
      "25/04/08 15:13:23 WARN DAGScheduler: Broadcasting large task binary with size 1104.8 KiB\n",
      "25/04/08 15:13:24 WARN DAGScheduler: Broadcasting large task binary with size 1164.0 KiB\n",
      "25/04/08 15:13:36 WARN DAGScheduler: Broadcasting large task binary with size 1268.9 KiB\n",
      "25/04/08 15:13:38 WARN DAGScheduler: Broadcasting large task binary with size 1547.9 KiB\n",
      "25/04/08 15:13:40 WARN DAGScheduler: Broadcasting large task binary with size 1733.4 KiB\n",
      "25/04/08 15:13:47 WARN DAGScheduler: Broadcasting large task binary with size 1268.9 KiB\n",
      "25/04/08 15:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1547.9 KiB\n",
      "25/04/08 15:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1733.4 KiB\n",
      "25/04/08 15:13:53 WARN DAGScheduler: Broadcasting large task binary with size 1829.2 KiB\n",
      "25/04/08 15:13:54 WARN DAGScheduler: Broadcasting large task binary with size 1707.6 KiB\n",
      "25/04/08 15:13:55 WARN DAGScheduler: Broadcasting large task binary with size 1044.9 KiB\n",
      "25/04/08 15:14:21 WARN DAGScheduler: Broadcasting large task binary with size 1090.0 KiB\n",
      "25/04/08 15:14:29 WARN DAGScheduler: Broadcasting large task binary with size 1090.0 KiB\n",
      "25/04/08 15:14:30 WARN DAGScheduler: Broadcasting large task binary with size 1160.1 KiB\n",
      "25/04/08 15:14:30 WARN DAGScheduler: Broadcasting large task binary with size 1069.5 KiB\n",
      "25/04/08 15:14:42 WARN DAGScheduler: Broadcasting large task binary with size 1240.2 KiB\n",
      "25/04/08 15:14:43 WARN DAGScheduler: Broadcasting large task binary with size 1500.5 KiB\n",
      "25/04/08 15:14:45 WARN DAGScheduler: Broadcasting large task binary with size 1693.9 KiB\n",
      "25/04/08 15:14:53 WARN DAGScheduler: Broadcasting large task binary with size 1240.2 KiB\n",
      "25/04/08 15:14:55 WARN DAGScheduler: Broadcasting large task binary with size 1500.5 KiB\n",
      "25/04/08 15:14:57 WARN DAGScheduler: Broadcasting large task binary with size 1693.9 KiB\n",
      "25/04/08 15:14:58 WARN DAGScheduler: Broadcasting large task binary with size 1810.8 KiB\n",
      "25/04/08 15:15:00 WARN DAGScheduler: Broadcasting large task binary with size 1771.0 KiB\n",
      "25/04/08 15:15:01 WARN DAGScheduler: Broadcasting large task binary with size 1349.4 KiB\n",
      "25/04/08 15:15:09 WARN DAGScheduler: Broadcasting large task binary with size 1278.1 KiB\n",
      "25/04/08 15:15:11 WARN DAGScheduler: Broadcasting large task binary with size 1572.8 KiB\n",
      "25/04/08 15:15:12 WARN DAGScheduler: Broadcasting large task binary with size 1783.4 KiB\n",
      "25/04/08 15:15:14 WARN DAGScheduler: Broadcasting large task binary with size 1914.6 KiB\n",
      "25/04/08 15:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1944.8 KiB\n",
      "25/04/08 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1432.9 KiB\n"
     ]
    }
   ],
   "source": [
    "rf_cv = crossval_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dea92a93-83ab-48b1-b34d-f1ff184da4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9726035535459465, dict_values([10, 5])],\n",
       " [0.640050100271086, dict_values([10, 10])],\n",
       " [0.6406749199863866, dict_values([10, 15])],\n",
       " [0.9001581177754586, dict_values([30, 5])],\n",
       " [0.6106290766809306, dict_values([30, 10])],\n",
       " [0.6105919325883014, dict_values([30, 15])],\n",
       " [0.8961704139505494, dict_values([50, 5])],\n",
       " [0.5927476104743807, dict_values([50, 10])],\n",
       " [0.5925069611108009, dict_values([50, 15])]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_list = []\n",
    "for i in range(len(paramGrid_rf)):\n",
    "    rf_list.append([rf_cv.avgMetrics[i], paramGrid_rf[i].values()])\n",
    "rf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3a0248c-ee04-4cfc-89c7-bddc4d6c65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model parameters\n",
    "best_index_rf = rf_cv.avgMetrics.index(min(rf_cv.avgMetrics))\n",
    "best_params_rf = paramGrid_rf[best_index_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0853ce19-1563-490f-b0da-cefde1ae7da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 15:16:44 WARN DAGScheduler: Broadcasting large task binary with size 1278.1 KiB\n",
      "25/04/08 15:16:46 WARN DAGScheduler: Broadcasting large task binary with size 1572.8 KiB\n",
      "25/04/08 15:16:49 WARN DAGScheduler: Broadcasting large task binary with size 1783.4 KiB\n",
      "25/04/08 15:16:50 WARN DAGScheduler: Broadcasting large task binary with size 1914.6 KiB\n",
      "25/04/08 15:16:52 WARN DAGScheduler: Broadcasting large task binary with size 1944.8 KiB\n",
      "25/04/08 15:16:53 WARN DAGScheduler: Broadcasting large task binary with size 1432.9 KiB\n"
     ]
    }
   ],
   "source": [
    "# Train best model on the full dataset\n",
    "rf_final = pipeline_rf.copy(best_params_rf).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceee623-26d4-4769-9b1f-5588120a8468",
   "metadata": {},
   "source": [
    "## Model Testing  \n",
    "In this section, final predictions were made using the trained models for each regression approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23574cb5-b49b-4d74-9e85-2686e34d4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a common evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Heating_Load\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7587517-191d-4d13-bd7b-5f986c2e2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using each model\n",
    "pred_lr = lr_final.transform(test_data)\n",
    "pred_dt = dt_final.transform(test_data)\n",
    "pred_rf = rf_final.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee2ec088-df9a-478c-8ba9-7f863e819d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3533647548048124 0.57934717838548 0.49263459516465913\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE for each model\n",
    "rmse_lr = evaluator.evaluate(pred_lr)\n",
    "rmse_dt = evaluator.evaluate(pred_dt)\n",
    "rmse_rf = evaluator.evaluate(pred_rf)\n",
    "\n",
    "print(rmse_lr, rmse_dt, rmse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00d3b1-aca0-44c5-b696-858497b4e77c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e243fa-07e9-48c1-bc0e-a01af4b102af",
   "metadata": {},
   "source": [
    "Among the three models evaluated, the Random Forest model with added polynomial and interaction features demonstrated the best performance, achieving the lowest RMSE of 0.49.\n",
    "This indicates that the model was able to capture complex, non-linear relationships between building design features and heating load more effectively than the other models.\n",
    "In contrast, the Linear Regression model, which assumes linearity, showed the highest RMSE (3.35), suggesting it may not fully capture the underlying patterns in the data.\n",
    "The Regression Tree performed better than the linear model (RMSE 0.58), but the ensemble nature of Random Forest, combined with feature expansion, provided further improvements by reducing overfitting and increasing robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b7a87-44bb-4fa5-af86-142579da8111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
